# Bellman equations and solving MDPs
Markov Decision Processes bring in the sequential decision making and delayed reward aspects of RL.

1. [Stanford CS234 lecture 2](https://www.youtube.com/watch?v=E3f2Camj0Is&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u&index=2)
2. [Professor Balaraman Ravindran's RL week 3,4 and 5th only till policy iteration](https://nptel.ac.in/courses/106106143/)
3. [Sutton and Barton chapter 3 and 4](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)

Balaram's lectures are more of classical RL and math intensive, So it is better to watch CS234 first.

You should implement the algorithms once you understand the proofs.   
I have implemented the following :

- [x] Value iteration
- [x] Policy iteration
- [x] Asynchronous value iteration
- [ ] Real time dynamic programming

The MDP which I have used is from the example 3.5 - Gridworld from Sutton and Barton.

These are my notes on these topics.
* [Bellman Equation](https://hackmd.io/Fuhp2hwyR4GknchLGBGTWw)
* [Bellman Optimality Equation](https://hackmd.io/wqQyQAvlTVeGzLsaVLUswg)
* [Value Iteration](https://hackmd.io/3o8W1o4rS6ikMs42PVXPAw)
* [Policy Iteration](https://hackmd.io/8F3m-j59TB-RaxP3ysVsCg?both)


References:
1. If you want to make your own blob environment then you can watch this [sentdex tutorial](https://www.youtube.com/watch?v=G92TF4xYQcU&list=PLQVvvaa0QuDezJFIOU5wDdfy4e9vdnx-7&index=4).
